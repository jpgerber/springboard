{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving movie recommendations for movie snobs (draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project examines how to improve the precision of recommender systems for movie snobs.\n",
    "\n",
    "I will go through the following tasks:\n",
    "1. Collate the 1,001 Movies list\n",
    "2. Select a dataset that has most of the 1,001 Movies in it\n",
    "3. Do some exploratory analyses of differences between 1,001 and non-1,001 movies (and The Dekalog and The Dark Knight), potentially by using demographic information\n",
    "4. Do some baseline recommender modeling\n",
    "5. Run a baseline model and then a model with 1,001 as a topic/rating.\n",
    "6. Compare the models (with a 10% holdout set)\n",
    "7. Try various models until we can predict the difference between 1,001 and non-1,001 movies \n",
    "\n",
    "Other ideas:\n",
    "1. Work out how long it takes for something to be canonical and then use this as a bias adjustment.\n",
    "2. use rotten tomatoes api to find difference b/w dek and dk reviews (base it on the mini-project for naive bayes section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five datasets of relevance on the Grouplens website.\n",
    "\n",
    "| Name | Movies | Ratings | Userdata | Release date |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| New research | 27K | 20M | None | 2016 |\n",
    "| Education (small) | 9K | 100K | None | 2018 |\n",
    "| Education (large) | 58K | 27M | None | 2018 |\n",
    "| Older (100K) | 1.7K | 100K| age, gender, occupation, zip | 1998 |\n",
    "| Older (1M) | 4K  | 1M | age, gender, occupation, zip | 2003 |\n",
    "\n",
    "The latter two are useful for identifying clusters of people but are a bit small, while the second and fourth are best if trying to identify features of movies themselves.\n",
    "\n",
    "The 1,001 movie file had 1,222 entries due to additions and deletions across the different editions..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packagesused\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests, zipfile, io\n",
    "# Importing the ratings data\n",
    "# First download and extract the files (there's a bunch so use a list and loop)\n",
    "#list_of_urls = ['http://files.grouplens.org/datasets/movielens/ml-latest.zip']\n",
    "#for url in list_of_urls:\n",
    "#    ratings_small_file = requests.get(url)\n",
    "#    z = zipfile.ZipFile(io.BytesIO(ratings_small_file.content))\n",
    "#    z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Importing the 1,001 list and converting it to a list\n",
    "snob_url = 'https://1001films.fandom.com/wiki/The_List'\n",
    "snob_text= requests.get(snob_url)\n",
    "soup = BeautifulSoup(snob_text.content, 'html.parser')\n",
    "basic_list = (soup.body.find_all('b'))\n",
    "thousand_list = [item.text for item in basic_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title\n",
      "1  A Trip to the Moon (Le Voyage Dans La Lune) (1...\n",
      "2                     The Great Train Robbery (1903)\n",
      "3                       The Birth of a Nation (1915)\n",
      "4                                Les Vampires (1915)\n",
      "5                                 Intolerance (1916)\n",
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1222 entries, 1 to 1222\n",
      "Data columns (total 1 columns):\n",
      "title    1222 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 19.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Here I get all three files into dataframes\n",
    "# first, read the correct downloaded csvs (from separate subfolders)\n",
    "ed_large_movies = pd.read_csv('ml-latest/movies.csv', sep = ',', header = 0)\n",
    "\n",
    "# The two older files have a different compression structure and require a different technique\n",
    "#older_small_movies = pd.read_csv('ml-100k/movies.csv', sep = ',', header = 0)\n",
    "#older_large_movies = pd.read_csv('ml-1m/movies.csv', sep = ',', header = 0)\n",
    "\n",
    "# Converting the 1,001 list to a dataframe and dropping the header row\n",
    "thousandone_movies = pd.DataFrame(thousand_list, columns = ['title']).drop(0)\n",
    "\n",
    "# The following code can be modified to check they are all dataframes\n",
    "print(thousandone_movies.head())\n",
    "print(ed_large_movies.head())\n",
    "thousandone_movies.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No doubt the hardest wrangling job was adjusting for any naming discrepancies in titles between files. I tried straight matching first joined the movie sets to the 1,001 list and the match rate was 49%. Then I tried some simple string manipulations and brought it up to 72%. Last, I realized that a tokenizer would account for many of the problems, and this lead to an 80% match rate (974 out of 1,222). This was acceptable for exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nlkt package had a tokenizer that helped match most of the titles. The tokenizer led to a 88% match rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the tokenizer version.\n",
    "# import the right package\n",
    "#import nltk\n",
    "# sometimes the line below needed to be included (not sure why)\n",
    "#from nltk import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "#import string\n",
    "# Make default stopwords a list of punctuation\n",
    "#stopwords = list(string.punctuation)\n",
    "# Add a few English words of no use (the default English list contains too many words)\n",
    "#stopwords.append('the')\n",
    "#stopwords.append('and')\n",
    "#print(stopwords)\n",
    "\n",
    "# a line of test code for my tokenizer\n",
    "#s = ['la voyage de la ( ) luns a trip to the moon','bread and milk . ,','la Voyage de la luns a trip to the moon']\n",
    "\n",
    "# Define tokenizer\n",
    "#def my_tokenizer(title_list, stopwords):\n",
    "#    '''This function takes a string and a list of stopwords and returns a list of word tokens\n",
    "#    excluding anything defined in stopwords'''\n",
    "#    # tokenize a lower-case string\n",
    "#    s_token = [word_tokenize(i.lower()) for i in title_list]\n",
    "#    # then filter out the stopwords\n",
    "#    s_filtered=[]\n",
    "#    for sentence in s_token:\n",
    "#        s_filt = [w for w in sentence if not w in stopwords]\n",
    "#        s_filtered.append(s_filt)\n",
    "#    # finally sort them\n",
    "#    s_sort = [sorted(item, key = lambda item:item[0]) for item in s_filtered] \n",
    "#    return s_sort\n",
    "\n",
    "# Call my function on two datasets\n",
    "#thousandone_movies['title_tokens'] = my_tokenizer(thousandone_movies['title'], stopwords)\n",
    "#ed_large_movies['title_tokens'] = my_tokenizer(ed_large_movies['title'], stopwords)\n",
    "\n",
    "# To make matching easy I turn them into a string (I kept the old list version in case I wanted\n",
    "#to use a matching algorithm)\n",
    "#thousandone_movies['title_string'] = thousandone_movies['title_tokens'].apply(', '.join)\n",
    "#ed_large_movies['title_string'] = ed_large_movies['title_tokens'].apply(', '.join)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I found a string matching library called fuzzywuzzy that does the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    A Trip to the Moon (Le Voyage Dans La Lune) (1...\n",
      "2                       The Great Train Robbery (1903)\n",
      "3                         The Birth of a Nation (1915)\n",
      "4                                  Les Vampires (1915)\n",
      "5                                   Intolerance (1916)\n",
      "6    The Cabinet of Dr. Caligari (Das Kabinett des ...\n",
      "Name: title, dtype: object\n",
      "Empty DataFrame\n",
      "Columns: [title_suggestion, percent_match]\n",
      "Index: []\n",
      "  title_suggestion percent_match                     0\n",
      "0              NaN           NaN  Vampires, Les (1915)\n",
      "1              NaN           NaN                    95\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "choicesed = ed_large_movies['title']\n",
    "titlestomatch = thousandone_movies['title']\n",
    "\n",
    "testtitles = titlestomatch.iloc[0:6]\n",
    "testtitle = titlestomatch.iloc[3]\n",
    "print(testtitles)\n",
    "def Matcher(title, choices):\n",
    "    title_match, percent_match, match3 = process.extractOne(title, choices)\n",
    "    return title_match, percent_match\n",
    "\n",
    "a = pd.DataFrame(columns=['title_suggestion','percent_match'])\n",
    "#np.empty(2, dtype=str)\n",
    "print(a)\n",
    "\n",
    "b = np.array(Matcher(testtitle, choicesed))\n",
    "a[0] = b\n",
    "print(a)\n",
    "#print(Matcher(testtitles, choicesed))\n",
    "#df[titlematches] = df[.apply(Matcher)\n",
    "#thousandone_movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-231f0622ada7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(a.type())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#dfb = pd.DataFrame(b, cols=['suggested_title','percent_match'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rows' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "b = pd.DataFrame(a, axis=)\n",
    "print(b)\n",
    "\n",
    "#print(a.type())\n",
    "#dfb = pd.DataFrame(b, cols=['suggested_title','percent_match'])\n",
    "#print(dfb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting some exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's review the variables we have for each movie. There are separate tables for movie (movieID, title, genres), movie rating (userID, movieID, rating, timestamp). tags (userID, movieID, tag, timestamp), and genome (a machine learned matrix output giving the relevance of difference keyword aggregates for each movie). These tables can be joined together but it will be easier to work with each table separately and apply the 1,001 list as a feature to each. First, a descriptive analysis of each table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings\n",
    "# Years\n",
    "\n",
    "# Year of release to rating band\n",
    "# Proportion of year of release to rating band\n",
    "# Number of ratings by year\n",
    "# Number of ratings since year of release\n",
    "\n",
    "#Genre\n",
    "# A simple split of all genres\n",
    "# Number of tags\n",
    "\n",
    "#Tags\n",
    "#Proportion of (Genomic) tags since year of release\n",
    "#Proportion of (Genomic) tags by number of ratings\n",
    "\n",
    "# Letter of alphabet in title (don't laugh, most good bands started with S in the 90s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some exploratory analyses of differences between 1,001 and non-1,001 movies (and The Dekalog and The Dark Knight), potentially by using demographic information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running a basic recommender model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate as tc #this library runs recommender systems\n",
    "\n",
    "training_data, validation_data = tc.recommender.util.random_split_by_user(actions, 'userId', 'movieId')\n",
    "baseline_model = tc.recommender.create(training_data, 'userId', 'movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_model = tc.recommender.create(training_data, 'userId', 'movieId', target='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparing the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer = tc.recommender.util.compare_models(validation_data, [baseline_model, ratings_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "References for turicreate:\n",
    "The basic user guide             https://apple.github.io/turicreate/docs/userguide/recommender/\n",
    "The recommender documentation    https://apple.github.io/turicreate/docs/api/turicreate.toolkits.recommender.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 974 entries, 1 to 1222\n",
      "Data columns (total 7 columns):\n",
      "title_x           974 non-null object\n",
      "title_tokens_x    974 non-null object\n",
      "title_string      974 non-null object\n",
      "movieId           974 non-null float64\n",
      "title_y           974 non-null object\n",
      "genres            974 non-null object\n",
      "title_tokens_y    974 non-null object\n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 60.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(ed_large_join.dropna().info())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
